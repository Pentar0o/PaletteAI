{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import math\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import cv2\n",
    "import gluoncv\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15,15)\n",
    "plt.rcParams['font.size'] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'DataSet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = glob.glob(data_dir+\"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} images\".format(len(train_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize(picfile, small_edge=512):\n",
    "    \n",
    "    \"\"\"resize image to a small edge of size small_edge\n",
    "       and save it at same name if smallest edge bigger than small_edge\"\"\"\n",
    "    \n",
    "    im = Image.open(picfile)\n",
    "    width, height = im.size\n",
    "    smallest = min(width, height)\n",
    "\n",
    "    ratio = smallest / small_edge\n",
    "    print('pic ' + picfile + ': applying ratio of ' + str(ratio))\n",
    "        \n",
    "    new_width, new_height = int(width/ratio), int(height/ratio)\n",
    "    print(new_width, new_height)\n",
    "    im2 = im.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "    cheminsauvegarde = 'ResizedPics/' + os.path.basename(picfile)\n",
    "    im2.save(cheminsauvegarde)\n",
    "        \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "resize_meta = {}\n",
    "\n",
    "for pic in train_images:\n",
    "    if pic.lower().endswith('.jpg'):\n",
    "        resize_meta[pic] = resize(pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 12\n",
    "cols = (int(math.sqrt(n_images)))*2\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for n, (image) in enumerate(train_images[:n_images]):\n",
    "    image = plt.imread(image)\n",
    "    a = fig.add_subplot(np.ceil(n_images/float(cols)), cols, n + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace=0.06, hspace=0.06)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning network for Trash detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have explored the dataset, let's get to work to be able to fine-tune our object detection model on this novel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Gluon Dataset \n",
    "\n",
    "We need to handle the data loading so that we can feed our network the images and the targets during training.\n",
    "\n",
    "We inherit from the base `Dataset` class from Gluon and create our own custom dataset that will return our images with the bounding box target information. We do a 80%, 15%, 5% split for training, validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluoncv as gcv\n",
    "from gluoncv.utils import viz\n",
    "\n",
    "from mxnet import gluon, nd, autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTDataset(gluon.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset to handle the TrashData Set\n",
    "    \"\"\"\n",
    "    def __init__(self, split='train', data_path=data_dir):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        data_path: str, Path to the data folder, default 'data'\n",
    "        split: str, Which dataset split to request, default 'train'\n",
    "    \n",
    "        \"\"\"\n",
    "        self.data_path = data_dir\n",
    "        self.image_info = []\n",
    "        with open(os.path.join('.', 'output.manifest'), errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                info = json.loads(line[:-1])\n",
    "                if len(info['Unknown']['annotations']):\n",
    "                    self.image_info.append(info)\n",
    "      \n",
    "        assert split in ['train', 'test', 'val']\n",
    "        \n",
    "        l = len(self.image_info)\n",
    "        if split == 'train':\n",
    "            self.image_info = self.image_info[:int(0.8*l)]\n",
    "        if split == 'val':\n",
    "            self.image_info = self.image_info[int(0.8*l):int(0.95*l)]\n",
    "        if split == 'test':\n",
    "            self.image_info = self.image_info[int(0.95*l):]\n",
    "\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        idx: int, index requested\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        image: nd.NDArray\n",
    "            The image \n",
    "        label: np.NDArray bounding box labels of the form [[x1,y1, x2, y2, class], ...]\n",
    "        \"\"\"\n",
    "        info = self.image_info[idx]\n",
    "        imagename = info['source-ref'].split('/')[-1]\n",
    "        image = mx.image.imread(os.path.join('ResizedPics', imagename))\n",
    "        boxes = info['Unknown']['annotations']\n",
    "        label = []\n",
    "        for box in boxes:\n",
    "            label.append([int(box['left']/resize_meta[os.path.join(data_dir, imagename)]),\n",
    "                          int(box['top']/resize_meta[os.path.join(data_dir, imagename)]),\n",
    "                          int((box['left']+box['width'])/resize_meta[os.path.join(data_dir, imagename)]),\n",
    "                          int((box['top']+box['height'])/resize_meta[os.path.join(data_dir, imagename)]),\n",
    "                          box['class_id']])\n",
    "        \n",
    "        return image, np.array(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only one class the \"bee\" class now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=[\"palette\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the dataset for each of the split. We will use the training split for training our model, the validation split to monitor our training for overfitting, and the testing split for the final qualitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GTDataset(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = GTDataset(split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GTDataset(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of bounding box label data [[x1,y1, x2, y2, class], ...] : {}\".format(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There is {} training images, {} validation images, {} testing images\".format(len(train_dataset), len(validation_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we now respect the same format as all other object detection dataset in GluonCV, we can take advantage of the vizualization functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = test_dataset[random.randint(0, len(test_dataset) - 1)]\n",
    "ax = viz.plot_bbox(image, bboxes=label[:, :4], labels=label[:, 4:5], class_names=classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD:  Single Shot multibox Detector\n",
    "\n",
    "SSD is a tried and tested model that gives us a good baseline for object detection. It is simple conceptually and fast and stable during training, that's why we pick it.\n",
    "Refer to this [graph](https://gluon-cv.mxnet.io/model_zoo/detection.html) for a complete comparison of object detectors on accuracy / speed / memory consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/1200/1*pPxrkm4Urz04Ez65mwWE9Q.png)\n",
    "\n",
    "*source: [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325), Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, 2015*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import some useful function from GluonCV:\n",
    "The SSD default transforms for training (a lot of data augmentation) and the validation transform for resizing and normalization\n",
    "The VOC07MApMetric to track the quality of the detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "from gluoncv.data.transforms.presets.ssd import SSDDefaultTrainTransform, SSDDefaultValTransform\n",
    "from gluoncv.utils.metrics.voc_detection import VOC07MApMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "image_size = 512\n",
    "num_workers = 8\n",
    "num_epochs = 100\n",
    "ctx = [mx.gpu(0)] if mx.context.num_gpus() > 0 else [mx.cpu()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gluoncv.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gcv.model_zoo.get_model('ssd_512_resnet50_v1_coco', pretrained=True)\n",
    "net.reset_class(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate ahead of the time the targets for the difference between the anchor box and the ground truth bounding boxes, for that we need the anchor boxes\n",
    "\n",
    "**Training data iterator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.train_mode():\n",
    "    _, _, anchors = net(mx.nd.zeros((1, 3, image_size, image_size)))\n",
    "train_transform = SSDDefaultTrainTransform(image_size, image_size, anchors)\n",
    "batchify_fn = Tuple(Stack(), Stack(), Stack())  # stack image, cls_targets, box_targets\n",
    "train_data = gluon.data.DataLoader(train_dataset.transform(train_transform), batch_size, True, batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation data iterator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = SSDDefaultValTransform(image_size, image_size)\n",
    "batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
    "val_data = gluon.data.DataLoader(validation_dataset.transform(val_transform), batch_size, False, batchify_fn=batchify_fn, last_batch='keep', num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a learning rate schedule, to decrease the learning rate by 3 after 5, 10 and 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_epochs = [50,70,80,90]\n",
    "iterations_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
    "steps_iterations = [s*iterations_per_epoch for s in steps_epochs]\n",
    "print(\"Learning rate drops after iterations: {}\".format(steps_iterations))\n",
    "schedule = mx.lr_scheduler.MultiFactorScheduler(step=steps_iterations, factor=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move the network to the right compute context and set the trainer with the right optimizer and learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().reset_ctx(ctx)\n",
    "trainer = gluon.Trainer(\n",
    "    net.collect_params(), 'sgd',\n",
    "    {'learning_rate': 0.0001, 'wd': 0.0004, 'momentum': 0.9, 'lr_scheduler':schedule})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object detection tasks combines losses for box localization and class detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbox_loss = gcv.loss.SSDMultiBoxLoss()\n",
    "ce_metric = mx.metric.Loss('CrossEntropy')\n",
    "smoothl1_metric = mx.metric.Loss('SmoothL1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the mean average precision, with the IoU (intersection over union) threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, val_data, ctx, classes, size):\n",
    "    \"\"\"\n",
    "    Compute the mAP for the network on the validation data\n",
    "    \"\"\"\n",
    "    metric = VOC07MApMetric(iou_thresh=0.5, class_names=classes)\n",
    "    net.set_nms(0.2)\n",
    "    for ib, batch in enumerate(val_data):\n",
    "        \n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        det_bboxes, det_ids, det_scores = [],[],[]\n",
    "        gt_bboxes,gt_ids = [], []\n",
    "        \n",
    "        for x, y in zip(data, label):\n",
    "            ids, scores, bboxes = net(x)\n",
    "            det_ids.append(ids)\n",
    "            det_scores.append(scores)\n",
    "            det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))\n",
    "            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n",
    "            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n",
    "            \n",
    "            metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids[0], None)\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**main training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#On cree le tableau pour stocker les mAP et les afficher dans un graphique\n",
    "TabHistorymAP = []\n",
    "TabEpoch = []\n",
    "\n",
    "best_val = 0 \n",
    "for epoch in range(num_epochs):\n",
    "    net.hybridize(static_alloc=True, static_shape=True)\n",
    "    #net.cast('float16')\n",
    "    ce_metric.reset()\n",
    "    smoothl1_metric.reset()\n",
    "    tic, btic = time.time(), time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_data):\n",
    "\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        with autograd.record():\n",
    "            cls_preds, box_preds = [], []\n",
    "            for x in data:\n",
    "                cls_pred, box_pred, _ = net(x)\n",
    "                cls_preds.append(cls_pred)\n",
    "                box_preds.append(box_pred)\n",
    "            sum_loss, cls_loss, box_loss = mbox_loss(cls_preds, box_preds, cls_targets, box_targets)\n",
    "            autograd.backward(sum_loss)\n",
    "\n",
    "        trainer.step(1)\n",
    "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
    "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
    "        name1, loss1 = ce_metric.get()\n",
    "        name2, loss2 = smoothl1_metric.get()\n",
    "\n",
    "        #if i % 20 == 0:\n",
    "        #    print('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}'.format(epoch, i, batch_size/(time.time()-btic), name1, loss1, name2, loss2))\n",
    "        btic = time.time()\n",
    "        \n",
    "    name, val = validate(net, val_data, ctx, classes, image_size)\n",
    "    \n",
    "    #A priori on a la moyenne des mAP des classes directement à la fin du tableau\n",
    "    meanAP = val[0]\n",
    "\n",
    "    #On stock l'hisotrique des résultats par Epoch\n",
    "    TabHistorymAP.append(meanAP)\n",
    "    TabEpoch.append(epoch)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    plt.plot(TabEpoch, TabHistorymAP, color='black', label='mAP')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show() # affiche la figure a l'ecran\n",
    "    \n",
    "    \n",
    "    print('[Epoch {}] Training cost: {:.3f}, Learning rate {}, mAP={:.3f}'.format(epoch, (time.time()-tic), trainer.learning_rate, val[0]))\n",
    "    \n",
    "    # If validation accuracy improve, save the parameters\n",
    "    if val[0] > best_val:\n",
    "        net.save_parameters('ssd_resnet.palette.params')\n",
    "        best_val = val[0]\n",
    "        print(\"Saving the parameters, best mAP {}\".format(best_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's test our model on the set aside testing images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_parameters('ssd_resnet.palette.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.set_nms(0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info in test_dataset.image_info:\n",
    "    test_path = os.path.join(data_dir,info['source-ref'].split('/')[-1])\n",
    "    x, image = gcv.data.transforms.presets.ssd.load_test(test_path, image_size)\n",
    "    cid, score, bbox = net(x.as_in_context(ctx[0]))\n",
    "    ax = viz.plot_bbox(image, bbox[0], score[0], cid[0], class_names=classes, thresh=0.6)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
